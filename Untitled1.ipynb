{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f3e17-6360-465d-98bd-473aee10f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminou\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "def download_folder(bucket_name, s3_folder, local_dir=None):\n",
    "    \"\"\"\n",
    "    Download the contents of a folder directory\n",
    "    Args:\n",
    "        bucket_name (str): Name of S3 bucket\n",
    "        s3_folder (str): S3 folder path to download\n",
    "        local_dir (str): Local directory path to save the files to\n",
    "    \"\"\"\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "        target = obj.key if local_dir is None \\\n",
    "            else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))\n",
    "        if not os.path.exists(os.path.dirname(target)):\n",
    "            os.makedirs(os.path.dirname(target))\n",
    "        if obj.key[-1] == '/':\n",
    "            continue\n",
    "        bucket.download_file(obj.key, target)\n",
    "\n",
    "# Download a folder from S3 bucket\n",
    "bucket_name = 'kagglexrayunziped'\n",
    "s3_folder = 'chest_xray/chest_xray/train'\n",
    "local_dir = '/home/studio-lab-user/sagemaker-studiolab-notebooks/train'\n",
    "download_folder(bucket_name, s3_folder, local_dir)\n",
    "print(\"terminou\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0277d-621a-453c-a449-e3d2c2162a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_folder = 'chest_xray/chest_xray/test'\n",
    "local_dir = '/home/studio-lab-user/sagemaker-studiolab-notebooks/test'\n",
    "download_folder(bucket_name, s3_folder, local_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e395e38-3cc6-4184-855e-91c78942a0eb",
   "metadata": {},
   "source": [
    "## Criar replicas nao rotacionadas de imagens com Pneumonia (oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1500ad-9fbe-4e02-8347-5676758cfcab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminou\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "folder_path = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train/PNEUMONIA\"\n",
    "\n",
    "def rotate_images(folder_path):\n",
    "    # Define the rotation angles\n",
    "    angles = [0]\n",
    "\n",
    "    # Loop through all the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a .jpeg image\n",
    "        if filename.endswith(\".jpeg\"):\n",
    "            # Open the image and rotate it\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(img_path)\n",
    "            for angle in angles:\n",
    "                rotated_img = TF.rotate(img, angle)\n",
    "\n",
    "                # Save the rotated image with a new filename\n",
    "                new_filename = f\"rotated_{angle}_{filename}\"\n",
    "                new_img_path = os.path.join(folder_path, new_filename)\n",
    "                rotated_img.save(new_img_path)\n",
    "\n",
    "rotate_images('/home/studio-lab-user/sagemaker-studiolab-notebooks/train/PNEUMONIA')\n",
    "print(\"terminou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ee1448-81a4-4786-bac7-1e2c277dd5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 150, 150]) 0\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# define the transformations to apply to each image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(150),\n",
    "    transforms.CenterCrop(150),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# create the dataset\n",
    "dataset = datasets.ImageFolder(root='/home/studio-lab-user/sagemaker-studiolab-notebooks/train', transform=transform)\n",
    "\n",
    "# get an image and its corresponding label\n",
    "img, label = dataset[0]\n",
    "\n",
    "# print the shape of the image and the label\n",
    "print(img.shape, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93b5c3-c0eb-477f-a552-9c83f3102658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install scikit-image==0.18.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ae76b9-850b-4d80-bf89-42a5f236af0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root='/home/studio-lab-user/sagemaker-studiolab-notebooks/train', transform=transform)\n",
    "\n",
    "# Define the GLCM properties to extract\n",
    "distances = [1, 2, 3]\n",
    "angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "properties = ['contrast', 'energy', 'homogeneity', 'correlation']\n",
    "\n",
    "# Loop through the dataset\n",
    "for i in range(len(dataset)):\n",
    "    # Load the image and convert to grayscale\n",
    "    image, label = dataset[i]\n",
    "    image = Image.fromarray(np.uint8(image.numpy().transpose(1, 2, 0) * 255))\n",
    "    image = image.convert('L')\n",
    "    \n",
    "    # Compute the GLCM\n",
    "    glcm = greycomatrix(np.array(image), distances, angles, symmetric=True, normed=True)\n",
    "    \n",
    "    # Compute the GLCM properties\n",
    "    features = np.hstack([greycoprops(glcm, prop).ravel() for prop in properties])\n",
    "    \n",
    "    # Save the features to file\n",
    "    filename = os.path.join('/home/studio-lab-user/sagemaker-studiolab-notebooks/glcm_features', f'{i}.npy')\n",
    "    np.save(filename, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029ad758-4a75-4019-8ba3-e1aee0074ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_88/553580958.py\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Compute the GLCM properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgreycoprops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Append the tensorized image and label to the lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_88/553580958.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Compute the GLCM properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgreycoprops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Append the tensorized image and label to the lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/skimage/feature/texture.py\u001b[0m in \u001b[0;36mgreycoprops\u001b[0;34m(P, prop)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'contrast'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dissimilarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'homogeneity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_over_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root='/home/studio-lab-user/sagemaker-studiolab-notebooks/train', transform=transform)\n",
    "\n",
    "# Define the GLCM properties to extract\n",
    "distances = [1, 2, 3]\n",
    "angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "properties = ['contrast', 'energy', 'homogeneity', 'correlation']\n",
    "\n",
    "# Create an empty list to store the tensorized images and labels\n",
    "tensorized_images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through the dataset\n",
    "for i in range(len(dataset)):\n",
    "    # Load the image and convert to grayscale\n",
    "    image, label = dataset[i]\n",
    "    image = Image.fromarray(np.uint8(image.numpy().transpose(1, 2, 0) * 255))\n",
    "    image = image.convert('L')\n",
    "    \n",
    "    # Compute the GLCM\n",
    "    glcm = greycomatrix(np.array(image), distances, angles, symmetric=True, normed=True)\n",
    "    \n",
    "    # Compute the GLCM properties\n",
    "    features = np.hstack([greycoprops(glcm, prop).ravel() for prop in properties])\n",
    "    \n",
    "    # Append the tensorized image and label to the lists\n",
    "    tensorized_images.append(torch.Tensor(features))\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert the lists to tensors\n",
    "tensorized_images = torch.stack(tensorized_images)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class TensorImageFolder(Dataset):\n",
    "    def __init__(self, tensorized_images, labels, transform=None):\n",
    "        self.images = tensorized_images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset_glcm = datasets.ImageFolder(tensorized_images, labels, transform=transform)\n",
    "\n",
    "# Create a data loader for the dataset\n",
    "data_loader_glcm = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c150edaa-d353-4853-be72-3df011e58d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Define a custom dataset class\n",
    "class TensorImageDataset(Dataset):\n",
    "    def __init__(self, tensorized_images, labels, transform=None):\n",
    "        self.images = tensorized_images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset_glcm = TensorImageDataset(tensorized_images, labels, transform=transform)\n",
    "\n",
    "# Create a data loader for the dataset\n",
    "data_loader_glcm = DataLoader(dataset_glcm, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242751d9-e805-4b02-ac96-06eaaa5aebbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TensorImageDataset at 0x7f055072db20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_glcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6065607-4a5c-4b7e-94fc-7949b55b12f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 1903 0.2953231739358907\n"
     ]
    }
   ],
   "source": [
    "sum_label = 0\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "    img, label = dataset[i]\n",
    "    sum_label += label\n",
    "\n",
    "print(sum_label,len(dataset),sum_label/len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b897c981-967f-4465-bac1-b9ef07da9e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test = datasets.ImageFolder(root='/home/studio-lab-user/sagemaker-studiolab-notebooks/test', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a2245cc8-a8d9-47d6-a6a3-e02cb50e97bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 150, 150]) 1\n",
      "390 624 0.625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get an image and its corresponding label\n",
    "img, label = dataset_test[500]\n",
    "\n",
    "# print the shape of the image and the label\n",
    "print(img.shape, label)\n",
    "\n",
    "sum_label = 0\n",
    "for i in range(len(dataset_test)):\n",
    "    \n",
    "    img, label = dataset_test[i]\n",
    "    sum_label += label\n",
    "\n",
    "print(sum_label,len(dataset_test),sum_label/len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b65d76e4-f516-449a-bf00-6e5196daa4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follwing classes are there : \n",
      " ['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "print(\"Follwing classes are there : \\n\",dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "202af2a3-56c6-41e3-aa69-791f5468015e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor is not a torch image.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_88/1883085278.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#display the first image in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdisplay_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset_glcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_88/160453063.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    474\u001b[0m             )\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0m_assert_image_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36m_assert_image_tensor\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_image_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tensor_a_torch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor is not a torch image.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor is not a torch image."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_img(img,label):\n",
    "    print(f\"Label : {dataset.classes[label]}\")\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "#display the first image in the dataset\n",
    "display_img(*dataset_glcm[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6295abda-a365-421d-a2c0-3cba6ff5d4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Define a custom dataset class\n",
    "class TensorImageDataset(Dataset):\n",
    "    def __init__(self, tensorized_images, labels, transform=None):\n",
    "        self.images = tensorized_images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.fromarray(np.uint8(self.images[index] * 255))\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset_glcm = TensorImageDataset(tensorized_images, labels, transform=transform)\n",
    "\n",
    "# Create a data loader for the dataset\n",
    "data_loader_glcm = DataLoader(dataset_glcm, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test displaying the image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_img(img, label):\n",
    "    print(f\"Label: {label}\")\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "\n",
    "# Display the first image in the dataset\n",
    "display_img(*dataset_glcm[902])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "519c7570-96e1-4bb7-89d3-60f7cba9766d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.folder.ImageFolder"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ccc6674-a5b7-4015-b04b-7375313f8449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c3cc560-e5d8-48d8-9f94-7edd584c2bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2429c4f0-d675-4126-89b5-ed1480cf6382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2392, 0.2353, 0.2314,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.2745, 0.2745, 0.2706,  ..., 0.0745, 0.0706, 0.0706],\n",
       "          [0.3490, 0.3020, 0.3098,  ..., 0.0706, 0.0706, 0.0667],\n",
       "          ...,\n",
       "          [0.6235, 0.6314, 0.6353,  ..., 0.3765, 0.2941, 0.2549],\n",
       "          [0.6275, 0.6314, 0.6392,  ..., 0.3529, 0.2902, 0.2549],\n",
       "          [0.6275, 0.6353, 0.6471,  ..., 0.3294, 0.2824, 0.2510]],\n",
       " \n",
       "         [[0.2392, 0.2353, 0.2314,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.2745, 0.2745, 0.2706,  ..., 0.0745, 0.0706, 0.0706],\n",
       "          [0.3490, 0.3020, 0.3098,  ..., 0.0706, 0.0706, 0.0667],\n",
       "          ...,\n",
       "          [0.6235, 0.6314, 0.6353,  ..., 0.3765, 0.2941, 0.2549],\n",
       "          [0.6275, 0.6314, 0.6392,  ..., 0.3529, 0.2902, 0.2549],\n",
       "          [0.6275, 0.6353, 0.6471,  ..., 0.3294, 0.2824, 0.2510]],\n",
       " \n",
       "         [[0.2392, 0.2353, 0.2314,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.2745, 0.2745, 0.2706,  ..., 0.0745, 0.0706, 0.0706],\n",
       "          [0.3490, 0.3020, 0.3098,  ..., 0.0706, 0.0706, 0.0667],\n",
       "          ...,\n",
       "          [0.6235, 0.6314, 0.6353,  ..., 0.3765, 0.2941, 0.2549],\n",
       "          [0.6275, 0.6314, 0.6392,  ..., 0.3529, 0.2902, 0.2549],\n",
       "          [0.6275, 0.6353, 0.6471,  ..., 0.3294, 0.2824, 0.2510]]]),\n",
       " 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "29f6bece-d0fe-4ad0-9a5b-3a42392036f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Data : 1903\n",
      "Length of Validation Data : 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 32\n",
    "val_size = 0\n",
    "train_size = len(dataset) - val_size \n",
    "\n",
    "train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 12034\n",
    "#Length of Validation Data : 2000\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(dataset, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(dataset_test, 624, num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f358fe89-8bd8-4fa5-8bb8-76b8580ad1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "val_size = 400\n",
    "train_size = len(dataset_glcm) - val_size \n",
    "\n",
    "train_data,val_data = random_split(dataset_glcm,[train_size,val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "147fe778-60a6-4b75-93a0-061909b8c29e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d39480-3bdd-4a7e-a7a5-58a0adfeed3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, labels in val_dl:\n",
    "    print(images.shape, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6728078d-e2b1-47d2-9b86-aa58cbf5d26f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "803c95ee-3899-4c57-8a89-8d52b84ee29e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(data_loader_glcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8622d79f-1049-4c6e-91f2-a77c243ccbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        preds = torch.argmax(out, dim=1)      # Get predicted labels\n",
    "        recall = recall_score(labels.cpu(), preds.cpu(), average='macro')  # Calculate recall\n",
    "        #print(\"Recall:\",recall)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc,'val_recall':recall}\n",
    "       \n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        batch_recalls = [x['val_recall'] for x in outputs]\n",
    "        epoch_recalls = torch.stack(batch_losses).mean()   # Combine Recalls\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(),'val_recall':epoch_recalls.item()}\n",
    "\n",
    "\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f},val_recall: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc'],result['val_recall']))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e60460-0e25-4d13-9278-71d58924838a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaturalSceneClassification(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(82944,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a235675-44a5-4837-9d83-4e4b600908ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    #print(outputs)\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a706391-3f4f-401f-a899-0dbc9354aa59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NaturalSceneClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047092f-57a2-45d7-baa8-624914568faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.6984, val_loss: 0.6925, val_acc: 0.6250,val_recall: 0.6925\n",
      "Epoch [1], train_loss: 0.6904, val_loss: 0.6972, val_acc: 0.3750,val_recall: 0.6972\n",
      "Epoch [2], train_loss: 0.6834, val_loss: 0.7020, val_acc: 0.3750,val_recall: 0.7020\n",
      "Epoch [3], train_loss: 0.6771, val_loss: 0.7067, val_acc: 0.3750,val_recall: 0.7067\n",
      "Epoch [4], train_loss: 0.6713, val_loss: 0.7115, val_acc: 0.3750,val_recall: 0.7115\n"
     ]
    }
   ],
   "source": [
    "historico2 = fit(5, 0.001, model, train_dl, val_dl, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21a57401-d3e7-4c22-8e3f-91ddc26247fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.6857568621635437,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6857568621635437,\n",
       "  'train_loss': 0.692064642906189},\n",
       " {'val_loss': 0.674298107624054,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.674298107624054,\n",
       "  'train_loss': 0.6791235208511353},\n",
       " {'val_loss': 0.6636916399002075,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6636916399002075,\n",
       "  'train_loss': 0.6672363877296448},\n",
       " {'val_loss': 0.6538041234016418,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6538041234016418,\n",
       "  'train_loss': 0.6560449600219727},\n",
       " {'val_loss': 0.6445767283439636,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6445767283439636,\n",
       "  'train_loss': 0.6457051038742065},\n",
       " {'val_loss': 0.6358767747879028,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6358767747879028,\n",
       "  'train_loss': 0.6360012292861938},\n",
       " {'val_loss': 0.6277244091033936,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6277244091033936,\n",
       "  'train_loss': 0.6268671751022339},\n",
       " {'val_loss': 0.6199440360069275,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6199440360069275,\n",
       "  'train_loss': 0.6179910898208618},\n",
       " {'val_loss': 0.6125180721282959,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6125180721282959,\n",
       "  'train_loss': 0.6099317669868469},\n",
       " {'val_loss': 0.6053647994995117,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.6053647994995117,\n",
       "  'train_loss': 0.6016106605529785},\n",
       " {'val_loss': 0.5984938740730286,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5984938740730286,\n",
       "  'train_loss': 0.5940409302711487},\n",
       " {'val_loss': 0.5919923782348633,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5919923782348633,\n",
       "  'train_loss': 0.5869289636611938},\n",
       " {'val_loss': 0.5857338905334473,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5857338905334473,\n",
       "  'train_loss': 0.5794692039489746},\n",
       " {'val_loss': 0.5796304941177368,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5796304941177368,\n",
       "  'train_loss': 0.5722479820251465},\n",
       " {'val_loss': 0.5737230181694031,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5737230181694031,\n",
       "  'train_loss': 0.5657392740249634},\n",
       " {'val_loss': 0.5680873990058899,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5680873990058899,\n",
       "  'train_loss': 0.5596405267715454},\n",
       " {'val_loss': 0.5626307725906372,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5626307725906372,\n",
       "  'train_loss': 0.5531260371208191},\n",
       " {'val_loss': 0.5573604702949524,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5573604702949524,\n",
       "  'train_loss': 0.5474754571914673},\n",
       " {'val_loss': 0.5522230863571167,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5522230863571167,\n",
       "  'train_loss': 0.5411175489425659},\n",
       " {'val_loss': 0.5472350716590881,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5472350716590881,\n",
       "  'train_loss': 0.5354883074760437},\n",
       " {'val_loss': 0.5423038601875305,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5423038601875305,\n",
       "  'train_loss': 0.5297861099243164},\n",
       " {'val_loss': 0.5374261736869812,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5374261736869812,\n",
       "  'train_loss': 0.523581862449646},\n",
       " {'val_loss': 0.5326360464096069,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5326360464096069,\n",
       "  'train_loss': 0.518103301525116},\n",
       " {'val_loss': 0.5279541611671448,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5279541611671448,\n",
       "  'train_loss': 0.512912929058075},\n",
       " {'val_loss': 0.5233368277549744,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5233368277549744,\n",
       "  'train_loss': 0.507807731628418},\n",
       " {'val_loss': 0.5187875032424927,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5187875032424927,\n",
       "  'train_loss': 0.5016649961471558},\n",
       " {'val_loss': 0.5144373178482056,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5144373178482056,\n",
       "  'train_loss': 0.4969192445278168},\n",
       " {'val_loss': 0.5102651715278625,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5102651715278625,\n",
       "  'train_loss': 0.4914303719997406},\n",
       " {'val_loss': 0.5062789916992188,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5062789916992188,\n",
       "  'train_loss': 0.485541969537735},\n",
       " {'val_loss': 0.5026424527168274,\n",
       "  'val_acc': 0.808571457862854,\n",
       "  'val_recall': 0.5026424527168274,\n",
       "  'train_loss': 0.48135942220687866}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historico2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf5d1eb-624e-43c3-b7a7-5f17abe11e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
